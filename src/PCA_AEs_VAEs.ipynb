{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split # Optional: if you want distinct train/test\n",
    "import time\n",
    "\n",
    "print(\"Loading MNIST dataset using fetch_openml...\")\n",
    "# Load data from https://www.openml.org/d/554\n",
    "# It's generally faster than the older fetch_mldata\n",
    "# The data is returned as a Pandas DataFrame by default if available,\n",
    "# specify return_X_y=True to get numpy arrays directly.\n",
    "# as_frame=False ensures numpy arrays even if pandas is installed.\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='liac-arff')\n",
    "\n",
    "# Convert labels to integers\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1 (optional but common)\n",
    "# Alternatively, StandardScaler handles mean/variance\n",
    "# X = X / 255.0\n",
    "\n",
    "# --- Data Subsampling (Optional, speeds up processing/plotting) ---\n",
    "# Use a smaller subset for faster PCA fitting and visualization\n",
    "n_samples_to_use = 10000 # Adjust as needed\n",
    "random_indices = np.random.choice(X.shape[0], n_samples_to_use, replace=False)\n",
    "X_subset = X[random_indices]\n",
    "y_subset = y[random_indices]\n",
    "print(f\"Using a subset of {n_samples_to_use} samples.\")\n",
    "\n",
    "\n",
    "# --- Preprocessing: Scaling ---\n",
    "# PCA is sensitive to feature scaling. Standardize features (mean=0, std=1)\n",
    "print(\"Scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_subset) # Fit and transform the subset\n",
    "\n",
    "# --- PCA Parameters ---\n",
    "# Choose a number of components for reconstruction comparison (e.g., same as AE encoding_dim)\n",
    "encoding_dim_pca = 32\n",
    "n_components_viz = 2 # For 2D visualization\n",
    "\n",
    "# --- PCA for Reconstruction ---\n",
    "print(f\"\\nApplying PCA for Reconstruction (n_components={encoding_dim_pca})...\")\n",
    "start_time = time.time()\n",
    "pca_recon = PCA(n_components=encoding_dim_pca)\n",
    "# Fit PCA on the scaled data\n",
    "pca_recon.fit(X_scaled)\n",
    "# Transform data to the lower dimension\n",
    "X_pca_recon = pca_recon.transform(X_scaled)\n",
    "# Reconstruct data back to the original (scaled) dimension\n",
    "X_reconstructed_scaled = pca_recon.inverse_transform(X_pca_recon)\n",
    "# Inverse scale to get back to original pixel value range\n",
    "X_reconstructed = scaler.inverse_transform(X_reconstructed_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"PCA fitting and reconstruction took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"PCA Reconstruction Results:\")\n",
    "print(\"Original data shape:\", X_scaled.shape)\n",
    "print(\"PCA reduced data shape:\", X_pca_recon.shape)\n",
    "print(\"Reconstructed data shape:\", X_reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization: Original vs. Reconstructed ---\n",
    "n_display = 10\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.suptitle(f'PCA Reconstruction (n={encoding_dim_pca})', fontsize=16)\n",
    "for i in range(n_display):\n",
    "    # display original (from the subset used)\n",
    "    ax = plt.subplot(2, n_display, i + 1)\n",
    "    plt.imshow(X_subset[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n_display // 2:\n",
    "        ax.set_title(\"Original Images\")\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n_display, i + 1 + n_display)\n",
    "    plt.imshow(X_reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n_display // 2:\n",
    "        ax.set_title(\"Reconstructed Images\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Reconstructed images from PCA are often blurrier than those from AEs/VAEs\",\n",
    "      \"for the same number of latent dimensions due to PCA's linearity.\")\n",
    "\n",
    "\n",
    "# --- Explained Variance ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca_recon.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title(f'PCA Explained Variance (up to {encoding_dim_pca} components)')\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.axhline(0.95, color='red', linestyle='--', linewidth=1, label='95% Variance')\n",
    "plt.axhline(0.90, color='orange', linestyle='--', linewidth=1, label='90% Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "explained_var_total = np.sum(pca_recon.explained_variance_ratio_)\n",
    "print(f\"Total variance explained by {encoding_dim_pca} components: {explained_var_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PCA for 2D Visualization ---\n",
    "print(f\"\\nApplying PCA for 2D Visualization (n_components={n_components_viz})...\")\n",
    "pca_viz = PCA(n_components=n_components_viz)\n",
    "# Fit and transform the scaled data to 2 dimensions\n",
    "X_pca_viz = pca_viz.fit_transform(X_scaled) # Can refit or use transform if pca_recon had >= 2 components\n",
    "\n",
    "print(\"PCA 2D reduced data shape:\", X_pca_viz.shape)\n",
    "\n",
    "# --- Visualization: 2D Latent Space ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], c=y_subset, cmap='viridis', s=5) # Use subset labels\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('MNIST PCA Projection (2 Components)')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=list(np.unique(y_subset).astype(str)))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe 2D plot shows how PCA linearly separates the digit classes.\",\n",
    "      \"Observe the overlap between certain digits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "input_dim = 784  # MNIST images are 28x28 = 784 pixels\n",
    "encoding_dim = 32 # Dimension of the latent space (bottleneck)\n",
    "hidden_dim = 128 # Dimension of hidden layer\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Define the Autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, encoding_dim),\n",
    "            nn.ReLU(True) # Often ReLU is used here too, or linear\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid() # Sigmoid activation for output pixels in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Helper function to get latent representation\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "         # Helper function to decode from latent space\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Converts PIL image to tensor [C, H, W] and scales to [0, 1]\n",
    "    # No need to flatten here, we'll do it in the training loop or DataLoader\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "model = Autoencoder(input_dim, hidden_dim, encoding_dim).to(device)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss for normalized pixels\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Autoencoder...\")\n",
    "model.train() # Set model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # Flatten the images and move data to the appropriate device\n",
    "        data = data.view(data.size(0), -1).to(device) # Flatten [N, 1, 28, 28] to [N, 784]\n",
    "\n",
    "        # Forward pass\n",
    "        recon = model(data)\n",
    "        loss = criterion(recon, data) # Compare reconstruction with original data\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "             print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'====> Epoch: {epoch+1} Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: encode and decode images from test set\n",
    "print(\"Testing Autoencoder...\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "test_loss = 0\n",
    "# Get a batch of test data\n",
    "test_iterator = iter(test_loader)\n",
    "test_images, _ = next(test_iterator)\n",
    "test_images_flat = test_images.view(test_images.size(0), -1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode\n",
    "    encoded_representation = model.encode(test_images_flat)\n",
    "    # Decode\n",
    "    decoded_images = model.decode(encoded_representation)\n",
    "    # Calculate test loss on this batch (optional)\n",
    "    test_loss = criterion(decoded_images, test_images_flat).item()\n",
    "\n",
    "print(f\"AE Training complete. Example Test Batch Loss: {test_loss:.4f}\")\n",
    "print(\"Encoded representation shape:\", encoded_representation.shape)\n",
    "print(\"Decoded images shape:\", decoded_images.shape) # Should be [batch_size, 784]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Show original vs reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization (Optional: Show original vs reconstructed) ---\n",
    "n_display = 10\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(n_display):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n_display, i + 1)\n",
    "    plt.imshow(test_images[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n_display // 2:\n",
    "        ax.set_title(\"Original Images\")\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n_display, i + 1 + n_display)\n",
    "    plt.imshow(decoded_images[i].cpu().view(28, 28), cmap='gray') # Reshape back to 28x28\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n_display // 2:\n",
    "        ax.set_title(\"Reconstructed Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating why averaging latent points doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded = model.encode(test_images_flat)\n",
    "    decoded = model.decode(encoded)\n",
    "\n",
    "# --- Demonstrating why averaging latent points doesn't work ---\n",
    "# Select two random digits from test images\n",
    "idx1, idx2 = 2, 10  # You can change these\n",
    "z1 = encoded[idx1]\n",
    "z2 = encoded[idx2]\n",
    "\n",
    "# Compute the average in latent space\n",
    "z_avg = (z1 + z2) / 2\n",
    "\n",
    "# Decode the average\n",
    "decoded_avg = model.decode(z_avg.unsqueeze(0))\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Display first original image\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_images[idx1].cpu().squeeze(), cmap=\"gray\")\n",
    "ax.set_title(\"Original 1\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Display second original image\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.imshow(test_images[idx2].cpu().squeeze(), cmap=\"gray\")\n",
    "ax.set_title(\"Original 2\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Display decoded average\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.imshow(decoded_avg.cpu().detach().view(28, 28).numpy(), cmap=\"gray\")\n",
    "ax.set_title(\"Latent Space Mean\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_channels=1, latent_dim=20, num_classes=10):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # --- Encoder ---\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels + 1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        encoder_output_size = 64 * 7 * 7\n",
    "        self.fc_mu = nn.Linear(encoder_output_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(encoder_output_size, latent_dim)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        self.decoder_input = nn.Linear(latent_dim + num_classes, encoder_output_size)\n",
    "        self.decoder_unflatten = nn.Unflatten(1, (64, 7, 7))\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, input_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        label_map = labels.view(labels.size(0), 1, 1, 1).expand(-1, 1, 28, 28)\n",
    "        x_cond = torch.cat([x, label_map], dim=1)\n",
    "        h = self.encoder_conv(x_cond)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, labels):\n",
    "        z_cond = torch.cat([z, labels], dim=1)\n",
    "        h = self.decoder_input(z_cond)\n",
    "        h = self.decoder_unflatten(h)\n",
    "        x_recon = self.decoder_conv(h)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z, labels)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 2  # Dimensionality of the latent space\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "beta = 1.0       # Weight for KL divergence (beta=1 for standard VAE)\n",
    "num_epochs = 15  # Train for a bit longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Converts PIL image to tensor [C, H, W] and scales to [0, 1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer\n",
    "model = CVAE(latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function (Negative ELBO)\n",
    "def loss_function(x, x_reconstructed, mu, logvar, beta):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss = recon_loss + beta * kl_div\n",
    "    return total_loss, recon_loss, kl_div\n",
    "\n",
    "# Training loop\n",
    "print(\"Training CVAE...\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_epoch = 0\n",
    "    total_recon_loss_epoch = 0\n",
    "    total_kl_div_epoch = 0\n",
    "    for i, (x, labels) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=10).float().to(device)\n",
    "\n",
    "        x_reconstructed, mu, logvar = model(x, labels)\n",
    "        loss, recon_loss, kl_div = loss_function(x, x_reconstructed, mu, logvar, beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_epoch += loss.item()\n",
    "        total_recon_loss_epoch += recon_loss.item()\n",
    "        total_kl_div_epoch += kl_div.item()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_loss = loss.item() / len(x)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    avg_epoch_loss = total_loss_epoch / len(train_loader.dataset)\n",
    "    avg_epoch_recon = total_recon_loss_epoch / len(train_loader.dataset)\n",
    "    avg_epoch_kl = total_kl_div_epoch / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch+1} Average loss: {avg_epoch_loss:.4f} (Recon: {avg_epoch_recon:.4f}, KL: {avg_epoch_kl:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conditional Generation: Generate digits using specific labels ---\n",
    "print(\"Generating digit-conditioned samples from prior...\")\n",
    "model.eval()\n",
    "digits = [0, 5, 9]\n",
    "samples_per_digit = 3\n",
    "\n",
    "with torch.no_grad():\n",
    "    for digit in digits:\n",
    "        # Create one-hot encoded label tensor\n",
    "        labels = torch.tensor([digit] * samples_per_digit).to(device)\n",
    "        one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "        # Sample latent vectors from N(0, I)\n",
    "        z_sample = torch.randn(samples_per_digit, latent_dim).to(device)\n",
    "\n",
    "        # Decode conditioned on label\n",
    "        generated_images = model.decode(z_sample, one_hot_labels).cpu()\n",
    "\n",
    "        # Plot the generated images\n",
    "        fig, axes = plt.subplots(1, samples_per_digit, figsize=(samples_per_digit * 2, 2))\n",
    "        for i in range(samples_per_digit):\n",
    "            axes[i].imshow(generated_images[i].squeeze(), cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "        plt.suptitle(f\"CVAE Generated Samples for Digit {digit}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
