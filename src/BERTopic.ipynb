{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b65041d-9ce3-412a-b66e-83e02aff54b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 98 training screenplays\n",
      "Loaded 11 test screenplays\n"
     ]
    }
   ],
   "source": [
    "# screenplays\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "\n",
    "def load_screenplays_from_folder(folder_path):\n",
    "    screenplays = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                screenplays.append(text)\n",
    "                filenames.append(filename)\n",
    "    return screenplays, filenames\n",
    "\n",
    "# Define paths\n",
    "train_path = os.path.join(\"..\", \"data\", \"screenplays\", \"train\")\n",
    "test_path = os.path.join(\"..\", \"data\", \"screenplays\", \"test\")\n",
    "\n",
    "# Load data\n",
    "X_train, train_filenames = load_screenplays_from_folder(train_path)\n",
    "X_test, test_filenames = load_screenplays_from_folder(test_path)\n",
    "\n",
    "# Quick checks\n",
    "print(f\"Loaded {len(X_train)} training screenplays\")\n",
    "print(f\"Loaded {len(X_test)} test screenplays\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4ab7a-f319-4607-a93c-f63222fdf032",
   "metadata": {},
   "source": [
    "### LOAD WORDS TO REMOVE FROM SCREENPLAY\n",
    "- stopwords\n",
    "- 200 most common words in screenplays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44d77242-d03e-490b-aae3-6be647bb2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load character names\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import re\n",
    "\n",
    "def load_character_names(movie_title):\n",
    "    with open(f\"../data/movie_data/{movie_title.replace(' ', '_')}.json\", \"r\", encoding=\"utf-8\") as file2:\n",
    "        character_names = json.load(file2)\n",
    "        \n",
    "    characters = [dicty[\"character\"] for dicty in character_names[\"actors_characters\"]]\n",
    "    characters_cleaned = []\n",
    "    for char in characters:\n",
    "         names = char.split(\" \")\n",
    "         for name in names: \n",
    "             name = re.sub(r\"[^a-z]\", \"\", name.lower())\n",
    "             if name != \"\":\n",
    "                 characters_cleaned.append(name)\n",
    "\n",
    "    return characters_cleaned\n",
    "\n",
    "def load_words(top_n_common=50):  # previously 200\n",
    "    # Load word frequency list\n",
    "    with open(\"../data/other/word_frequencies.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        word_frequencies = json.load(file)\n",
    "\n",
    "    # Keep only top 50 frequent words instead of 200\n",
    "    common_words = list(word_frequencies.keys())[:top_n_common]\n",
    "\n",
    "    # Limit built-in stopwords — keep pronouns, conjunctions, etc.\n",
    "    minimal_stopwords = {\"the\", \"a\", \"an\", \"of\", \"and\", \"in\", \"on\", \"at\", \"to\"}\n",
    "\n",
    "    return common_words + list(minimal_stopwords)\n",
    "\n",
    "    \n",
    "words_to_remove = load_words(top_n_common=200)\n",
    "#characters = load_character_names(movie_title=\"Die Hard\")\n",
    "print(\"Words loaded.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfc4981-b342-4c79-b7bb-e5e7a4a22c1f",
   "metadata": {},
   "source": [
    "### CLEAN TEXT\n",
    "- Lowercase, remove non-letter characters\n",
    "- filter out common words and stopwords\n",
    "- filter out proper nouns and family names via NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a486dc-a7eb-416e-956b-60b3e299792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['father', 'sister', 'stepmom', 'grandma', 'mother', 'grandpa', 'uncle', 'mary', 'john', 'cousin', 'stepdad', 'brother', 'mom', 'dad', 'aunt']\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load spaCy NER model (do this once at the top of your notebook)\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # You can upgrade to en_core_web_trf if needed\n",
    "\n",
    "# Define family words that may not be tagged as entities\n",
    "FAMILY_TERMS = {\n",
    "    \"mom\", \"dad\", \"mother\", \"father\", \"sister\", \"brother\",\n",
    "    \"uncle\", \"aunt\", \"grandma\", \"grandpa\", \"cousin\", \"stepmom\", \"stepdad\"\n",
    "}\n",
    "\n",
    "def load_nouns(screenplay_text=None, movie_title=None):\n",
    "    \"\"\"\n",
    "    Extracts PERSON entities and family-related nouns from a screenplay.\n",
    "    Returns a list of lowercase words to remove.\n",
    "    \"\"\"\n",
    "    if screenplay_text is None:\n",
    "        raise ValueError(\"You must pass in screenplay_text for NER.\")\n",
    "\n",
    "    doc = nlp(screenplay_text)\n",
    "\n",
    "    # Extract named entities of type PERSON\n",
    "    names = {ent.text.lower() for ent in doc.ents if ent.label_ == \"PERSON\"}\n",
    "\n",
    "    # Add family-related words (case insensitive)\n",
    "    family_words = FAMILY_TERMS\n",
    "\n",
    "    # Merge and return as list\n",
    "    nouns_to_remove = list(names.union(family_words))\n",
    "    return nouns_to_remove\n",
    "\n",
    "\n",
    "def clean_and_chunk_text(screenplay_text, movie_title, top_n_common=200, verbose=True, chunk_size=35):\n",
    "    cleaned_words = screenplay_text.lower().replace(\"\\n\", \" \")                \n",
    "    cleaned_words = [re.sub(r\"[^a-zA-Z]\", \"\", word.strip()) for word in cleaned_words.split(\" \") if word.strip() != \"\"]\n",
    "    \n",
    "    # Filter out stop words, character names, and common words\n",
    "    words_to_remove = load_words(top_n_common=200)\n",
    "    names_to_remove = load_nouns(screenplay_text=screenplay_text, movie_title=movie_title)\n",
    "    filtered_words = [word for word in cleaned_words if word not in words_to_remove and word not in names_to_remove]\n",
    "\n",
    "    # filter out short words\n",
    "    filtered_words = [word for word in filtered_words if len(word) > 2]\n",
    "\n",
    "    # chunks\n",
    "    chunks = [\" \".join(filtered_words[i:i+chunk_size]) for i in range(0, len(filtered_words), chunk_size)]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Original words: {len(cleaned_words)}, Filtered words: {len(filtered_words)}\")\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "sample = \"\"\"\n",
    "INT. DINER – NIGHT\n",
    "\n",
    "JOHN sits across from MARY. His face is bruised.\n",
    "\n",
    "JOHN\n",
    "(quietly)\n",
    "I didn’t think I’d see you again.\n",
    "\n",
    "MARY\n",
    "You don’t look so good.\n",
    "\n",
    "The WAITRESS drops off the check.\n",
    "\"\"\"\n",
    "\n",
    "print(load_nouns(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada094c0",
   "metadata": {},
   "source": [
    "### RUN BERTOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9aa109e-412a-4690-9db8-2a7ab964c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "\n",
    "def create_plots(topic_model, label=\"train_corpus\", top_n_topics=10):\n",
    "    # barcharts\n",
    "    fig = topic_model.visualize_barchart(top_n_topics=top_n_topics)\n",
    "    fig.update_layout(title_text=f\"Top {top_n_topics} BERTopic distributions in {label}\", title_x=0.5)\n",
    "    fig.write_html(f\"plots/{label}_bertopic_barchart.html\")\n",
    "\n",
    "    # intertopic distances\n",
    "    fig2 = topic_model.visualize_topics()\n",
    "    fig2.update_layout(title_text=f\"Intertopic Distance Map for {label}\", title_x=0.5)\n",
    "    fig2.write_html(f\"plots/{label}_intertopic_distance.html\")\n",
    "\n",
    "    # dendrogram\n",
    "    fig3 = topic_model.visualize_hierarchy()\n",
    "    fig3.write_html(f\"plots/{label}_dendrogram.html\")\n",
    "\n",
    "    print(\"Plots saved.\")\n",
    "\n",
    "\n",
    "def create_model_and_plots(chunks, embeddings, label=\"train_corpus\"):\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=3,\n",
    "                                    min_samples=1,\n",
    "                                    cluster_selection_epsilon=0.1)\n",
    "\n",
    "    umap_model = UMAP(n_components=10, n_neighbors=15, min_dist=0.05, metric='cosine')\n",
    "\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "    topics, probs = topic_model.fit_transform(chunks, embeddings)\n",
    "\n",
    "    create_plots(topic_model, label=label, top_n_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07c609fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a319216412141ceae0f34f3f4e19f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 433.87 seconds\n",
      "Embeddings created!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "def overlapping_chunks(words, chunk_size=250, overlap=50):\n",
    "    step = chunk_size - overlap\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), step)]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunks = []\n",
    "\n",
    "for screenplay_text, filename in zip(X_train, train_filenames):\n",
    "    movie_title = filename.replace(\".txt\", \"\")\n",
    "    \n",
    "    if screenplay_text:\n",
    "        cleaned = clean_and_chunk_text(screenplay_text, movie_title, top_n_common=200, verbose=False, chunk_size=99999)\n",
    "        words = \" \".join(cleaned).split()\n",
    "        all_chunks = overlapping_chunks(words, chunk_size=250, overlap=50)\n",
    "\n",
    "        # Drop the first chunk if it looks like metadata\n",
    "        if len(all_chunks) > 1:\n",
    "            chunks += all_chunks[1:]\n",
    "        else:\n",
    "            chunks += all_chunks\n",
    "\n",
    "embeddings = embedder.encode(chunks, show_progress_bar=True, batch_size=256)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time: {round(end_time - start_time, 2)} seconds\\nEmbeddings created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd46e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 14:24:10,583 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-05 14:24:12,194 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-05 14:24:12,195 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-05 14:24:12,923 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-05 14:24:12,926 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-05 14:24:13,907 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4.05 seconds\n",
      "BERTopic trained!\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from bertopic import BERTopic\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=20,\n",
    "    min_samples=5,\n",
    "    cluster_selection_epsilon=0.05,\n",
    "    prediction_data=True  # <-- REQUIRED for topic_model to calculate probabilities\n",
    ")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_components=5,                # fewer dims = tighter clusters\n",
    "    n_neighbors=30,                # smoother local structure\n",
    "    min_dist=0.2,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(chunks, embeddings)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time: {round(end_time - start_time, 2)} seconds\\nBERTopic trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5381331",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d583834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score (avg top-10 word similarity): 0.0000\n",
      "Diversity Score (unique top-10 words): 0.9216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Get topic-word matrix\n",
    "topics = topic_model.get_topics()\n",
    "top_n = 10\n",
    "\n",
    "# Filter out -1 topic (outliers)\n",
    "valid_topics = [k for k in topics.keys() if k != -1]\n",
    "\n",
    "# 1. Topic Coherence (avg cosine sim of top words)\n",
    "def topic_coherence(topics_dict, top_n=10):\n",
    "    vectorizer = CountVectorizer()\n",
    "    coherence_scores = []\n",
    "    for topic in valid_topics:\n",
    "        words = [word for word, _ in topics_dict[topic][:top_n]]\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "        X = vectorizer.fit_transform(words).toarray()\n",
    "        sim = cosine_similarity(X)\n",
    "        upper_tri = sim[np.triu_indices_from(sim, k=1)]\n",
    "        coherence_scores.append(np.mean(upper_tri))\n",
    "    return np.mean(coherence_scores)\n",
    "\n",
    "# 2. Topic Diversity (fraction of unique words in top-N)\n",
    "def topic_diversity(topics_dict, top_n=10):\n",
    "    all_words = []\n",
    "    for topic in valid_topics:\n",
    "        all_words.extend([word for word, _ in topics_dict[topic][:top_n]])\n",
    "    unique_words = set(all_words)\n",
    "    return len(unique_words) / (top_n * len(valid_topics))\n",
    "\n",
    "coherence = topic_coherence(topics, top_n=top_n)\n",
    "diversity = topic_diversity(topics, top_n=top_n)\n",
    "\n",
    "print(f\"Coherence Score (avg top-{top_n} word similarity): {coherence:.4f}\")\n",
    "print(f\"Diversity Score (unique top-{top_n} words): {diversity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85386691",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "space odyssey screenplay stanley kubrick arthur clark hawk films ltd mgm studios boreham wood herts title part africa years ago views african drylands drought remorseless drought lasted ten million years million reign ter rible lizards since passed continent known africa battle survival reached climax ferocity victor yet sight dry barren land swift fierce flourish hope exist caves manapes field none attributes pathetic road racial extinction twenty occupied group caves overlooking parched valley divided sluggish brown stream tribe always hungry starving dim glow dawn creeps discovers died relationship beyond understanding emac iated feels akin sadness carries leaves hyenas among kind almost giant nearly five high though badly undernourished weighs hundred pounds hairy muscular quite manlike already nearer ape forehead low ridges eyesockets yet unmistakably genes promise humanity upon hostile world already gaze beyond grasp ape dark deepset dawning awarenessthe intima tions intelligence fulfill million years stream others dawn sky brightens tribe reach shallow stream others already less annoying eighteen impossible distinguish members moonwatchers tribe coming others begin angrily dance shriek stream reply kind confrontation lasts minutes display dies quickly begun everyone drinks fill muddy honor satisfied group staked claim territory african plain herbivores companions search berries fruit leaves fight pangs hunger competing samr fodder potential source food ever hope eat yet thousands tons meat roaming parched savanna brush beyond reach idea eating beyond imagination starving death midst plenty parched countryside lion tribe wanders bare flat country foraging roots occasional berries eight irregularly strung plain fifty apart ground flat miles becomes aware\n",
      "\n",
      "--- Chunk 1 ---\n",
      "competing samr fodder potential source food ever hope eat yet thousands tons meat roaming parched savanna brush beyond reach idea eating beyond imagination starving death midst plenty parched countryside lion tribe wanders bare flat country foraging roots occasional berries eight irregularly strung plain fifty apart ground flat miles becomes aware lion stalking yards defenceless nowhere hide scatter directions lion brings ground tree finds honey though real remembrance compare caves finds hive bees stump tree enjoys finest delicacy ever course also collects many stings scacely notices near contentment ever likely thought hungry actually weak hunger hominid hope caves terrors valley full rises cold wind blows distant mountains cold tonight cold hunger matter real concern merely part background sun shone gave warmth dangerous enemies abroad crawls clambers large boulder besides entrance squats survey valley hunting beast approached relative safety creatures ever lived earth moonwatchers race raise interest though remember reach try touch ghostly tree high enough stirs shrieks screams echo slope lower caves occasional growl lion happening darkness oneeye family dying thought might crosses moonwatchers harsh logic survival rules fancies silent lest attract disaster caves tortured spells fitful dozing fearful waiting gathered nightmares generations yet stream invasion others growing desperate forage valley almost exhausted perhaps realise moonwatchers tribe lost numbers choose mourning break truce meet river misty dawn deeper menacing note challenge noisy usually harmless confrontation lasts seconds invasion begins uncertainlymoving horde others cross river shieking threats hunched attack led bigtoothed hominid moonwatchers size age startled frightened tribe retreats advance throwing substantial\n",
      "\n",
      "--- Chunk 2 ---\n",
      "almost exhausted perhaps realise moonwatchers tribe lost numbers choose mourning break truce meet river misty dawn deeper menacing note challenge noisy usually harmless confrontation lasts seconds invasion begins uncertainlymoving horde others cross river shieking threats hunched attack led bigtoothed hominid moonwatchers size age startled frightened tribe retreats advance throwing substantial imprecations invaders mist rage confusion driven territory badness lose river death situation beyond experience becomes dimly aware others slowing advancing obvious reluctance uncertain unhappy become bigtooth retains original drive rapidly seperated followers moonwatchers morale immediately revives slows retreat begins reassuring noises companions novel sensations fill dim faint precursors bravery leadership realizes bigtooth tribes halt many paces disorganized unscientific conflict ended quickly either used fist club innovation lay hundreds thousands years future instead weakening fighters claw scratch try bite rolling patch stony ground reach top chance chooses grab hair bigtooths scalp bang ground resulting crack satisfactory produces immediate weakening tooths resistance quickly repeats bigtooth ceases watcher keeps exhilirating game shrieks panic others retreat stream defenders cautiously pursue far waters edge dozing fitfully weakened stuggle startled fetid darkness straining senses fear creeps soul already twice members species expect heard cats approached silence betrayed rare slide earth occasional cracking twig yet continuing crunching noise grows steadily louder seemed enormous beast moving making attempt concealment ignoring obstacles came possibly identified heard history planet rock rock leads tribe river almost forgotten terror happened initial noise associate strange danger fear least alarming cube fifteen made completely transparent material indeed easy except sun glints edges natural objects\n",
      "\n",
      "--- Chunk 3 ---\n",
      "noise grows steadily louder seemed enormous beast moving making attempt concealment ignoring obstacles came possibly identified heard history planet rock rock leads tribe river almost forgotten terror happened initial noise associate strange danger fear least alarming cube fifteen made completely transparent material indeed easy except sun glints edges natural objects compare apparition though wisely cautious things hesitate walk happens feels warm hard surface several minutes intense thought arrives brilliant explanation rock course must grown many plants pulpy things shaped pebbles seem shoot hours darkness true round whereas large square greater philosophers prepared overlook equally striking exceptions laws superb piece abstract thinking leads deduction immediately test round pebbleplants tasty though made violently sick perhaps square licks attempted nibbles quickly disillusion nourishment sensible hominid continues river forgets cube cube lesson hundred yards rock begins quite soft tracks stand paralyzed trail jaws hanging simple maddeningly repetitious rhythm pulses crystal cube hypnotises within spell million year drumming heard africa throbbing grows louder insistent presently hominids begin forward sleepwalkers source magnetic sometimes dancing blood responds rhythms descendants create ages yet totally entranced gather cube forgetting hardships perils approaching dusk hunger bellies spinning wheels begin merge spokes fuse luminous bars recede distance rotating axes hominids watch wide eyed mesmerized captives crystal cube magic though magical gone perfectly normal scene appears cubical block carved shifted block group four hominids might members moonwatchers tribe eating chunks meat carcass warthog lies near family male female children gorged replete sleek glossy pelts condition imagined stir lazily loll ease near entrance\n",
      "\n",
      "--- Chunk 4 ---\n",
      "wide eyed mesmerized captives crystal cube magic though magical gone perfectly normal scene appears cubical block carved shifted block group four hominids might members moonwatchers tribe eating chunks meat carcass warthog lies near family male female children gorged replete sleek glossy pelts condition imagined stir lazily loll ease near entrance apparently peace world spectacle domestic bliss merges totally different scene family longer reposing peacefully foraging searching food normal hominids warthog ambles group browsing humanoids without giving glance slightest danger species happy state affairs male bends picks heavy stone lying hurls upon unfortunate pig stone descends upon skull making exactly noise produced almost forgotten encounter bigtooth result warthog gives amazed indignant squeal collapses motionless heap whole sequence begins unfolds incredible slowness detail movement followed stone arches leisurely air pig crumples sinks ground scene freezes moments slayer standing motionless slain weapons scene fades cube glimmering outline darkness hominids stir awakening dream realise scuttle caves concious memory seen brooding entrance lair ears attuned noises world feels faint twinges potent emotion urge kill taken step humanity plains utopia babies born sometimes lived feeble toothless thirty yearolds died lion took toll others threatened daily river trib prospered course single year companions changed almost beyond recognition become plump family longer haunted dreams learned lessons handle stone tools weapons cube revealed longer halfnumbed starvation leisure rudiments thought casually accepted associate crystal cube standing utopia perfect blemishes marauding lion whose passion hominids seemed grown stronger better nourished second tribe river somehow others survived stubbornly refused die starvation caves\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"\\n--- Chunk {i} ---\\n{chunks[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd7f4cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 4406\n",
      "Embeddings shape: (4406, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7902ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
